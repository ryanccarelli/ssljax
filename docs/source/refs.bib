@misc{2020:chen,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
  year={2020},
  eprint={2002.05709},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{2020:chen2,
  title={Big Self-Supervised Models are Strong Semi-Supervised Learners},
  author={Ting Chen and Simon Kornblith and Kevin Swersky and Mohammad Norouzi and Geoffrey Hinton},
  year={2020},
  eprint={2006.10029},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
}

@misc{2021:zhai,
  title={Scaling Vision Transformers},
  author={Xiaohua Zhai and Alexander Kolesnikov and Neil Houlsby and Lucas Beyer},
  year={2021},
  eprint={2106.04560},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@misc{2019:henaff,
  title={Data-Efficient Image Recognition with Contrastive Predictive Coding},
  author={Olivier J. Hénaff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and Aaron van den Oord},
  year={2020},
  eprint={1905.09272},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@misc{2019:hjelm,
  title={Learning deep representations by mutual information estimation and maximization},
  author={R Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Phil Bachman and Adam Trischler and Yoshua Bengio},
  year={2019},
  eprint={1808.06670},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
}

@misc{2020:brown,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year={2020},
  eprint={2005.14165},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
}

@misc{2021:radford,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  year={2021},
  eprint={2103.00020},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
}

@misc{2008:collobert,
  author = {Ronan Collobert and Jason Weston},
  title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
  journal = {Proceedings of the 25th International Conference on Machine Learning, Helsinki, Finland},
  year = {2008},
}

@misc{2013:mikolov,
  title = {Efficient Estimation of Word Representations in Vector Space},
  author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  year={2013},
  eprint={1301.3781},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
}

@misc{2014:pennington,
  title = "{G}lo{V}e: Global Vectors for Word Representation",
  author = "Pennington, Jeffrey  and
    Socher, Richard  and
    Manning, Christopher",
  booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
  month = oct,
  year = "2014",
  address = "Doha, Qatar",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D14-1162",
  doi = "10.3115/v1/D14-1162",
  pages = "1532--1543",
}

@misc{2018:devlin,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year={2019},
  eprint={1810.04805},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
}

@misc{2019:liu,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year={2019},
  eprint={1907.11692},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
}

@misc{2019:conneau,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzmán and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
  year={2020},
  eprint={1911.02116},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
}

@misc{2020:grill,
  title={Bootstrap your own latent: A new approach to self-supervised Learning},
  author={Jean-Bastien Grill and Florian Strub and Florent Altché and Corentin Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and Bernardo Avila Pires and Zhaohan Daniel Guo and Mohammad Gheshlaghi Azar and Bilal Piot and Koray Kavukcuoglu and Rémi Munos and Michal Valko},
  year={2020},
  eprint={2006.07733},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
}

@misc{2019:he,
 title={Momentum Contrast for Unsupervised Visual Representation Learning},
 author={Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross Girshick},
 year={2020},
 eprint={1911.05722},
 archivePrefix={arXiv},
 primaryClass={cs.CV},
}

@misc{2019:misra,
  title={Self-Supervised Learning of Pretext-Invariant Representations},
  author={Ishan Misra and Laurens van der Maaten},
  year={2019},
  eprint={1912.01991},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
}

@misc{2021:caron,
  title={Emerging Properties in Self-Supervised Vision Transformers},
  author={Mathilde Caron and Hugo Touvron and Ishan Misra and Hervé Jégou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
  year={2021},
  eprint={2104.14294},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
}

@misc{2020:caron,
  title={Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
  author={Mathilde Caron and Ishan Misra and Julien Mairal and Priya Goyal and Piotr Bojanowski and Armand Joulin},
  year={2020},
  eprint={2006.09882},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
}

@misc{2021:assran,
  title={Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples},
  author={Mahmoud Assran and Mathilde Caron and Ishan Misra and Piotr Bojanowski and Armand Joulin and Nicolas Ballas and Michael Rabbat},
  year={2021},
  eprint={2104.13963},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
}

@INPROCEEDINGS{2005:chopra,
  author={Chopra, S. and Hadsell, R. and LeCun, Y.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  title={Learning a similarity metric discriminatively, with application to face verification},
  year={2005},
  volume={1},
  number={},
  pages={539-546 vol. 1},
  doi={10.1109/CVPR.2005.202},
}

@article{2015:chopra,
  title={FaceNet: A unified embedding for face recognition and clustering},
  url={http://dx.doi.org/10.1109/CVPR.2015.7298682},
  DOI={10.1109/cvpr.2015.7298682},
  journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher={IEEE},
  author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year={2015},
  month={Jun}
}

@InProceedings{2010:gutmann,
  title =   {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author =    {Gutmann, Michael and Hyvärinen, Aapo},
  booktitle =   {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages =     {297--304},
  year =    {2010},
  editor =    {Teh, Yee Whye and Titterington, Mike},
  volume =      {9},
  series =    {Proceedings of Machine Learning Research},
  address =     {Chia Laguna Resort, Sardinia, Italy},
  month =     {13--15 May},
  publisher =    {PMLR},
  pdf =   {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},
  url =     {https://proceedings.mlr.press/v9/gutmann10a.html},
  abstract =      {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.}
}

@misc{2014:dosovitskiy,
  title={Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks},
  author={Alexey Dosovitskiy and Philipp Fischer and Jost Tobias Springenberg and Martin Riedmiller and Thomas Brox},
  year={2015},
  eprint={1406.6909},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
}

@misc{2019:oord,
  title={Representation Learning with Contrastive Predictive Coding},
  author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},
  year={2019},
  eprint={1807.03748},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
}

@misc{2019:bachman,
  title={Learning Representations by Maximizing Mutual Information Across Views},
  author={Philip Bachman and R Devon Hjelm and William Buchwalter},
  year={2019},
  eprint={1906.00910},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
