env:
  seed: 1337
  numpy_seed: 1337
  jax_seed: 1337
  epochs: 2
  accum_steps: 1
  save_checkpoint:
    params:
      ckpt_dir: "outs/checkpoints/"
      keep: 1
      overwrite: True
      keep_every_n_steps: 300
  restore_checkpoint:
    params: {}
  half_precision: False
  dynamic_scale: False
  dynamic_scale_params: {}
  input_shape: (32,32,3)

trainer:
  name: SSLTrainer

model:
  name: SSLModel
  branches:
    0:
      name: Branch
      pipelines: ["0"]
      stages:
        stop_gradient: False
        # B/16 on imagenet
        body:
          module: ViT
          params:
            num_classes:
            # mlp_dim
            # Ti 768
            # S 1536
            # B 3072
            # L 4096
            # H 5120
            mlp_dim: 3072
            # num_layers
            # Ti 12
            # S 12
            # B 12
            # L 24
            # H 32
            num_layers: 12
            # num_heads
            # Ti 3
            # S 6
            # B 12
            # L 16
            # H 16
            num_heads: 12
            # patches
            # 16 for B/16 (in general after /)
            patches: 16
            # hidden_size
            # Ti 192
            # S 384
            # B 768
            # L 1024
            # H 1280
            hidden_size: 768
            representation_size: None
            dropout_rate: 0.1
            attention_dropout_rate: 0.
            stochastic_depth: None
            classifier: "token"
            # TODO: convert str to jnp type
            dtype: "float32"
        basehead:
          module: DINOMLP
          params:
            layer_dims: [2048, 2048, 256]
            dropout_prob: 0.0
            batch_norm: False
            batch_norm_params: {}
            activation_name: "gelu"
        dinohead:
          module: DINOHead
          params:
            # TODO: what is K?
            out_dim: 10
        proj:
          module: CenterSoftmax
          params: {}

    1:
      name: Branch
      pipelines: ["1"]
      stages:
        stop_gradient: True
        body:
          # from https://github.com/google-research/scenic/blob/main/scenic/projects/baselines/configs/imagenet/imagenet_vit_config.py
          module: ViT
          params:
            num_classes:
            # mlp_dim
            # Ti 768
            # S 1536
            # B 3072
            # L 4096
            # H 5120
            mlp_dim: 3072
            # num_layers
            # Ti 12
            # S 12
            # B 12
            # L 24
            # H 32
            num_layers: 12
            # num_heads
            # Ti 3
            # S 6
            # B 12
            # L 16
            # H 16
            num_heads: 12
            # patches
            # 16 for B/16 (in general after /)
            patches: 16
            # hidden_size
            # Ti 192
            # S 384
            # B 768
            # L 1024
            # H 1280
            hidden_size: 768
            representation_size: None
            dropout_rate: 0.1
            attention_dropout_rate: 0.
            stochastic_depth: None
            classifier: "token"
            # TODO: convert str to jnp type
            dtype: "float32"
        basehead:
          module: DINOMLP
          params:
            layer_dims: [2048, 2048, 256]
            dropout_prob: 0.0
            batch_norm: False
            batch_norm_params: {}
            activation_name: "gelu"
        dinohead:
          module: DINOHead
          params:
            # TODO: what is K?
            out_dim: 10
        proj:
          module: CenterSoftmax
          params: {}

loss:
  name: cross_entropy_loss
  params: {}

optimizers:
  branches:
    0:
      name: lars
      params:
        # TODO: check that this should be momentum (comes from deepmind's \beta)
        momentum: 0.9
        trust_coefficient: 1e-3
        weight_decay: 1.5e-6
        weight_decay_mask: True
        trust_ratio_mask: True

schedulers:
  branches:
    0:
      learning_rate:
        name: BYOLlr
        params:
          # note that learning rate scales w/ number of epochs see deepmind implementation
          base_learning_rate: 0.2
          warmup_steps: 1000
          batch_size: 4096
          # epochs * train_images_per_epoch // batch_size
          # TODO: for now this is a random number
          total_steps: 100000
  post_process:
    0:
      tau:
        name: BYOLema
        params:
          # TODO: why is this max_steps and total_steps above (inherit this weirdness from DM implementation)
          base_ema: .996
          max_steps: 100000

post_process:
  funcs:
    0:
      name: ema
      params:
        online_branch_name: branch_0
        target_branch_name: branch_1
        remove_from_online: [proj]

meter:
  name: SSLMeter

pipelines:
  flatten: False
  pre:
    augmentations:
      RandomFlip:
        params:
          prop: 1.0
  branches:
    0:
      augmentations:
        RandomFlip:
          params:
            prob: 1.0
        ColorTransform:
          params:
            prob: 1.0
            brightness: 0.4
            contrast: 0.4
            saturation: 0.2
            hue: 0.1
            color_jitter_prob: 0.8
            to_grayscale_prob: 0.2
            shuffle: True
        GaussianBlur:
          params:
            prob: 0.1
        Solarize:
          params:
            prob: 0.2
            threshold: 0.5
        Clip:
          params:
            prob: 1.0
            x_min: 0
            x_max: 1
    1:
      augmentations:
        RandomFlip:
          params:
            prob: 1.0
        ColorTransform:
          params:
            prob: 1.0
            brightness: 0.4
            contrast: 0.4
            saturation: 0.2
            hue: 0.1
            color_jitter_prob: 0.8
            to_grayscale_prob: 0.2
            shuffle: True
        GaussianBlur:
          params:
            prob: 1.0
        Clip:
          params:
            prob: 1.0
            x_min: 0
            x_max: 1

dataloader:
  name: CIFAR10
  params:
    batch_size: 8
